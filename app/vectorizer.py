import os
import pickle
import faiss
from sentence_transformers import SentenceTransformer
from app.data_loader import load_all_documents

# Cáº¥u hÃ¬nh
CHUNK_SIZE = 500
VECTOR_DIM = 384  # all-MiniLM-L6-v2 => 384 chiá»u
VECTOR_PATH = "vector_store/vectorized_data.pkl"

# Chunking Ä‘oáº¡n vÄƒn báº£n dÃ i thÃ nh cÃ¡c pháº§n nhá»


def chunk_text(text, chunk_size=CHUNK_SIZE):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

# NhÃºng vÄƒn báº£n vÃ  lÆ°u FAISS index


def vectorize_and_save(docs):
    # BÆ°á»›c 1: Chunk toÃ n bá»™ vÄƒn báº£n
    chunks = []
    for doc in docs:
        chunks.extend(chunk_text(doc))

    print(f"ğŸ”¢ Tá»•ng sá»‘ Ä‘oáº¡n vÄƒn báº£n sau khi chunking: {len(chunks)}")

    # BÆ°á»›c 2: Táº¡o embedding
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(chunks, show_progress_bar=True)

    # BÆ°á»›c 3: Táº¡o FAISS index
    index = faiss.IndexFlatL2(VECTOR_DIM)
    index.add(embeddings)

    # BÆ°á»›c 4: LÆ°u index + chunks
    os.makedirs(os.path.dirname(VECTOR_PATH), exist_ok=True)
    with open(VECTOR_PATH, "wb") as f:
        pickle.dump({"faiss_index": index, "chunks": chunks}, f)

    print(f"âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u vector vÃ o {VECTOR_PATH}")


# Entry point
if __name__ == "__main__":
    print("ğŸ“„ Äang táº£i tÃ i liá»‡u...")
    docs = load_all_documents("data")

    if not docs:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u trong thÆ° má»¥c 'data/'. HÃ£y thÃªm file .txt, .docx, hoáº·c .html.")
    else:
        vectorize_and_save(docs)
